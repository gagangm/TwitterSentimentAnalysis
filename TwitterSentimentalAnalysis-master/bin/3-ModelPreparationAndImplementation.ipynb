{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQTPskj1g-Ri8Bb5tiVKNaJtGjM4nKFZ84SaHM78R2amyGwL3Ok\" alt=\"roundtoc\" style=\"float:left;width:40px;height:40px;\">   <b>Configuration Block</b><a class=\"anchor\" id=\"H0\"></a></h2>  \n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note:\tWhen multiple configuration files will be provided and they might have some duplicate keys        \n",
      "\tthen then the priority to the config will be provided to the one configwhich was provided        \n",
      "\tat the start of the \"config_file_paths_li\".\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/Documents/JobWorkSpace/TwitterSentimentAnalysis-RickyTim/TwitterSentimentalAnalysis/data/inputs/training.1600000.processed.noemoticon.csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from AML00_GetConfig import ProcessConfigDict, Configuration\n",
    "from AML00_DevelopConfigDict import config as config_dict\n",
    "\n",
    "stillworkingwithconfig = True\n",
    "if stillworkingwithconfig:\n",
    "    config_instance = ProcessConfigDict(config_dict)\n",
    "    # config_instance.get_original_config_dict()\n",
    "    # config_instance.get_modified_config_dict()\n",
    "    # print(config_instance.get_config_json())\n",
    "    config_instance.write_json_config()\n",
    "    config_instance.write_ini_config()\n",
    "\n",
    "conf = Configuration(['../config/configuration_json.json'], True,True)\n",
    "conf.load_config_files()\n",
    "#print(conf.get_keys_in_config())\n",
    "conf.get_config('paths', 'raw_training_data_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://blockclub.co/wp-content/uploads/2017/07/blockclub_brandmark_circle_rgb.jpg\" alt=\"roundtoc\" style=\"float:left;width:50px;height:50px;\"><b>&emsp;1. Importing the Libraries</b><a class=\"anchor\" id=\"H1\"></a></h2>\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txt0_GeneralFunc import (\n",
    "    custom_ast_lit_eval, levprint, print_system_info, generate_highlighted_heading, \n",
    "    get_abs_path_from_relative_path, time_cataloging, create_key, add_recommendation\n",
    ")\n",
    "\n",
    "from txt2_EDA_general import (\n",
    "    plot_wordcloud, plot_chartext_informations, cnt_words, zipf_law_plot, \n",
    "    visualize_word_count_wrt_class\n",
    ")\n",
    "\n",
    "print_system_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Description: \n",
    "    This file provide some function that are for general use cases.\n",
    "Function this file Contains:\n",
    "    - levprint: It is used to print statement based on the level. i.e.  function level.\n",
    "    - print_system_info: Used to print the system configurations\n",
    "    - generate_highlighted_heading: Used to generate some comment based heading / seperators\n",
    "Package Installation:\n",
    "# !pip3 install seaborn\n",
    "Reference Links:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html\n",
    "'''\n",
    "\n",
    "# --------------------------------------------  Loading Libraries  --------------------------------------------- #\n",
    "import time, os, glob, ast, re, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "# pd.set_option('display.height', 10000)\n",
    "pd.set_option('display.max_colwidth', 180)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline \n",
    "# %matplotlib notebook\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "from pylab import *\n",
    "# pip3 install bokeh\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import output_notebook, show\n",
    "output_notebook()\n",
    "from bokeh.models import HoverTool\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction import text\n",
    "\n",
    "# start_time = time.time()\n",
    "# print('Time taken {:.4f} s'.format(end_time - start_time))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://blockclub.co/wp-content/uploads/2017/07/blockclub_brandmark_circle_rgb.jpg\" alt=\"roundtoc\" style=\"float:left;width:50px;height:50px;\"><b>&emsp;2. Loading the Cleaned Data</b><a class=\"anchor\" id=\"H2\"></a></h2>\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF = pd.read_csv(config['paths']['cleaned_train_data_file'], sep='\\t')\n",
    "test_DF = pd.read_csv(config['paths']['cleaned_test_data_file'], sep='\\t')\n",
    "\n",
    "print(\"Training Dataset Shape: \", train_DF.shape)\n",
    "print(\"Test Dataset Shape: \", test_DF.shape)\n",
    "display(train_DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://blockclub.co/wp-content/uploads/2017/07/blockclub_brandmark_circle_rgb.jpg\" alt=\"roundtoc\" style=\"float:left;width:50px;height:50px;\"><b>&emsp;3. Understanding the Data</b><a class=\"anchor\" id=\"H3\"></a></h2>\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://blockclub.co/wp-content/uploads/2017/07/blockclub_brandmark_circle_rgb.jpg\" alt=\"roundtoc\" style=\"float:left;width:50px;height:50px;\"><b>&emsp;4. Data Preprocessing</b><a class=\"anchor\" id=\"H4\"></a></h2>\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://blockclub.co/wp-content/uploads/2017/07/blockclub_brandmark_circle_rgb.jpg\" alt=\"roundtoc\" style=\"float:left;width:50px;height:50px;\"><b>&emsp;5.  Data Exploration and Visualization</b><a class=\"anchor\" id=\"H5\"></a></h2>\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)  \n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;**Text Visualization**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"https://blockclub.co/wp-content/uploads/2017/07/blockclub_brandmark_circle_rgb.jpg\" alt=\"roundtoc\" style=\"float:left;width:50px;height:50px;\"><b>&emsp;6.  Model Development</b><a class=\"anchor\" id=\"H6\"></a></h2>\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)  \n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;**To classsify text as Positive or Negative** \n",
    "  \n",
    "### 6.1. Dataset Split\n",
    "Before we can train any model, we first consider how to split the data. Here I chose to split the data into three chunks: train, development, test. I referenced Andrew Ng's \"deeplearning.ai\" course on how to split the data.\n",
    "\n",
    "    Train set: The sample of data used for learning\n",
    "    Development set (Hold-out cross-validation set): The sample of data used to tune the parameters of a classifier, and provide an unbiased evaluation of a model.\n",
    "    Test set: The sample of data used only to assess the performance of a final model.\n",
    "\n",
    "The ratio I decided to split my data is 98/1/1, 98% of data as the training set, and 1% for the dev set, and the final 1% for the test set. The rationale behind this ratio comes from the size of my whole data set. The dataset has more than 1.5 million entries. In this case, only 1% of the whole data gives me more than 15,000 entries. This is more than enough to evaluate the model and refine the parameters.\n",
    "\n",
    "Another approach is splitting the data into only train and test set, and run k-fold cross-validation on the training set, so that you can have an unbiased evaluation of a model. But considering the size of the data, I have decided to use the train set only to train a model, and evaluate on the dev set, so that I can quickly test different algorithms and run this process iteratively.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import train_val_test_split\n",
    "\n",
    "\n",
    "## Splitting the dataset in three ways\n",
    "train_df, val_df, test_df = train_val_test_split(train_DF, split_val=config['ModelPreparation']['Train:Val:Test_Split'])\n",
    "\n",
    "## Adding More Observation from the actual testset to test set\n",
    "test_df = test_df.append(test_DF, ignore_index=True).sample(frac=1).reset_index(drop=True)  \n",
    "\n",
    "print('\\t\\tTotal Entries\\t\\tNegativeObs%\\tPositiveObs%')\n",
    "print('Train set:\\t\\t{0}\\t\\t{1:.2f}%\\t\\t{2:.2f}%'.format(\n",
    "    len(train_df), (sum(train_df['sentiment_class']==0)/(len(train_df)*1.))*100, (sum(train_df['sentiment_class']==1)/(len(train_df)*1.))*100))\n",
    "print(\"Validation set:\\t\\t{0}\\t\\t{1:.2f}%\\t\\t{2:.2f}%\".format(\n",
    "    len(val_df), (sum(val_df['sentiment_class']==0)/(len(val_df)*1.))*100, (sum(val_df['sentiment_class']==1)/(len(val_df)*1.))*100))\n",
    "print(\"Test set:\\t\\t{0}\\t\\t{1:.2f}%\\t\\t{2:.2f}%\".format(\n",
    "    len(test_df), (sum(test_df['sentiment_class']==0)/(len(test_df)*1.))*100, (sum(test_df['sentiment_class']==1)/(len(test_df)*1.))*100))\n",
    "\n",
    "\n",
    "# print(train_df.shape, other_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Baseline Accuracy\n",
    "\n",
    "When comparing various machine learning algorithms, baseline provides a point of reference to compare. The most popular baseline is the Zero Rule (ZeroR). ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods. As you can see from the above validation set class division, the majority class is negative with 50.40%, which means if a classifier predicts negative for every validation data, it will get 50.40% accuracy.\n",
    "\n",
    "Another baseline I wanted to compare the validation results with is TextBlob. **Textblob is a python library for processing textual data. Apart from other useful tools such as POS tagging, n-gram, The package has built-in sentiment classification. This is a so-called out-of-the-box sentiment analysis tool**, and in addition to the null accuracy, I will also keep in mind of the accuracy I get from TextBlob sentiment analysis to see how my model is performing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "## Pretrained Model\n",
    "tb_pred_probability = [ TextBlob(i).sentiment.polarity for i in val_df['text'] ]\n",
    "tb_pred_deter = [0 if n < 0 else 1 for n in tb_pred_probability]\n",
    "print('\\t\\tTotal Entries\\t\\tNegativeObs%\\tPositiveObs%')\n",
    "print(\"Validation set:\\t\\t{0}\\t\\t{1:.2f}%\\t\\t{2:.2f}%\".format(\n",
    "    len(tb_pred_deter), sum([ele ==0 for ele in tb_pred_deter ])/len(tb_pred_deter)*100, sum([ele ==1 for ele in tb_pred_deter ])/len(tb_pred_deter)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import plotConfusionMatrix\n",
    "\n",
    "plotConfusionMatrix(val_df['sentiment_class'], tb_pred_deter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Parameter Tuning\n",
    "\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "If we want to use text in machine learning algorithms, we’ll have to **convert them to a numerical representation**. One of the methods is called bag-of-words approach. The bag of words model ignores grammar and order of words. Once we have a corpus (text data) then first, a list of vocabulary is created based on the entire corpus. Then each document or data entry is represented as numerical vectors based on the vocabulary built from the corpus.\n",
    "\n",
    "### Count Vectorizer\n",
    "\n",
    "With count vectorizer, we merely count the appearance of the words in each text. For example, let's say we have 3 documents in a corpus: \"I love dogs\", \"I hate dogs and knitting\", \"Knitting is my hobby and my passion\". If we build vocabulary from these three sentences and represent each document as count vectors, it will look like below pictures.\n",
    "\n",
    "**title**\n",
    "\n",
    "But if the size of the corpus gets big, the number of vocabulary gets too big to process. With my 1.5 million tweets, if I build vocabulary without limiting the number of vocabulary, I will have more than 260,000 vocabularies. This means that the shape of training data will be around 1,500,000 x 260,000, this sounds too big to train various different models with. So I decided to limit the number of vocabularies, but I also wanted to see how the performance varies depending on the number of vocabularies.\n",
    "\n",
    "Another thing I wanted to explore is stopwords. Stop Words are words which do not contain important significance, such as \"the\", \"of\", etc. It is often assumed that removing stopwords is a necessary step, and will improve the model performance. But I wanted to see for myself if this is really the case. So I ran the same test with and without stop words and compared the result. In addition, I also defined my custom stopwords list, which contains top 10 most frequent words in the corpus: \"to\", \"the\", \"my\", \"it\", \"and\", \"you\", \"not\", \"is\", \"in\", \"for\".\n",
    "\n",
    "A model I chose to evaluate different count vectors is the logistic regression. It is one of the linear models, so computationally scalable to big data, compared to models like KNN or random forest. And once I have the optimal number of features and make a decision on whether to remove stop words or not, then I will try different models with the chosen number of vocabularies' count vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the evaluation result, removing stop words did not improve the model performance, but keeping the stop words yielded better performance. I wouldn't say that removing stopwords are not helping the model performance every time, but as empirical findings, in this particular setting, keeping the stop words improve the model performance.\n",
    "\n",
    "### Bigram\n",
    "According to Wikipedia, \"n-gram is a contiguous sequence of n items from a given sequence of text or speech\". In other words, n-grams are simply all combinations of adjacent words or letters of length n that you can find in your source text. Below picture represents well how n-grams are constructed out of source text.\n",
    "\n",
    "![](https://i.stack.imgur.com/8ARA1.png)\n",
    "\n",
    "![](https://i.stack.imgur.com/ysM0Z.png)\n",
    "![](https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png)\n",
    "![](https://classeval.files.wordpress.com/2015/06/error-rate.png?w=440&h=155)\n",
    "\n",
    "\n",
    "In this project, I will extend the bag-of-words to trigrams, and see if it affects the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualise the results we got from unigram, bigram, and trigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The best validation set accuracy for each n-gram is as below.\n",
    "\n",
    "    unigram: 80,000 & 90,000 features at validation accuracy 80.28%\n",
    "    bigram: 70,000 features at validation accuracy 82.25%\n",
    "    trigram: 80,000 features at validation accuracy 82.44%\n",
    "\n",
    "Below I defined another function to take a closer look at best performing number of features with each n-gram. Below function not only reports accuracy but also gives confusion matrix and classification report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TFIDF Vectorizer\n",
    "\n",
    "TF-IDF is another way to convert textual data to a numeric form and is short for Term Frequency-Inverse Document Frequency. The vector value it yields is the product of these two terms; TF and IDF.\n",
    "\n",
    "Let's first look at Term Frequency. We have already looked at term frequency above with count vectorizer, but this time, we need one more step to calculate the relative frequency. Let's say we have two documents in total as below.\n",
    "\n",
    "    I love dogs\n",
    "    I hate dogs and knitting\n",
    "\n",
    "Relative term frequency is calculated for each term within each document as below.  \n",
    "$${TF(t,d)} = \\frac {number\\ of\\ times\\ term(t)\\ appears\\ in\\ document(d)}{total\\ number\\ of\\ terms\\ in\\ document(d)}$$\n",
    "\n",
    "For example, if we calculate relative term frequency for 'I' in both document 1 and document 2, it will be as below.  \n",
    "$${TF('I',d1)} = \\frac {1}{3} \\approx {0.33}$$$${TF('I',d2)} = \\frac {1}{5} = {0.2}$$\n",
    "\n",
    "Next, we need to get Inverse Document Frequency, which measures how important a word is to differentiate each document by following the calculation as below.\n",
    "$${IDF(t,D)} = \\log \\Big(\\frac {total\\ number\\ of\\ documents(D)}{number\\ of\\ documents\\ with\\ the\\ term(t)\\ in\\ it}\\Big)$$\n",
    "\n",
    "If we calculate inverse document frequency for 'I',\n",
    "$${IDF('I',D)} = \\log \\Big(\\frac {2}{2}\\Big) = {0}$$\n",
    "\n",
    "Once we have the values for TF and IDF, now we can calculate TFIDF as below.\n",
    "$${TFIDF(t,d,D)} = {TF(t,d)}\\cdot{IDF(t,D)}$$\n",
    "\n",
    "Following the case of our example, TFIDF for term 'I' in both documents will be as below.\n",
    "$${TFIDF('I',d1,D)} = {TF('I',d1)}\\cdot{IDF('I',D)} = {0.33}\\times{0} = {0}$$$${TFIDF('I',d2,D)} = {TF('I',d2)}\\cdot{IDF('I',D)} = {0.2}\\times{0} = {0}$$\n",
    "\n",
    "As you can see, the term 'I' appeared equally in both documents, and the TFIDF score is 0, which means the term is not really informative in differentiating documents. The rest is same as count vectorizer, TFIDF vectorizer will calculate these scores for terms in documents, and convert textual data into a numeric form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms Comparison\n",
    "\n",
    "\n",
    "The best result I can get with logistic regression was by using TFIDF vectorizer of 100,000 features including up to trigram. With this I will first fit various different models and compare their validation results, then I will build an ensemble (voting) classifier with top 5 models.\n",
    "\n",
    "I haven't included some of computationally expensive models, such as KNN, random forest, considering the size of data and the scalability of models. And the fine-tuning of models will come after I try some other different vectorisation of textual data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Packages related to Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "## Importing Packages related to Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "# pip3 install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "## Not Used\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "\n",
    "\n",
    "## Importing Other Packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VectorozersModels_dict = {\n",
    "    'CountVectorizer': {\n",
    "        'ModelType': 'VectorizerModel', 'Model': CountVectorizer(), 'DataTypeBoundation': 'StringOfText',\n",
    "        'fit': True, 'fit_predict': False, 'predict': False, 'fit_transform': True, 'Transform': True,\n",
    "        'inverse_transform': True, 'DecisionFunction': False, 'decode': True, 'get_feature_names': True, \n",
    "        'get_stop_words': True\n",
    "        }, \n",
    "    'TfidfVectorizer': {\n",
    "        'ModelType': 'VectorizerModel', 'Model': TfidfVectorizer(), 'DataTypeBoundation': 'StringOfText',\n",
    "        'fit': True, 'fit_predict': False, 'predict': False, 'fit_transform': True, 'Transform': True,\n",
    "        'inverse_transform': True, 'DecisionFunction': False, 'decode': True, 'get_feature_names': True, \n",
    "        'get_stop_words': True\n",
    "        },\n",
    "    'HashingVectorizer': {\n",
    "        'ModelType': 'VectorizerModel', 'Model': HashingVectorizer(), 'DataTypeBoundation': 'StringOfText',\n",
    "        'fit': True, 'fit_predict': False, 'predict': False, 'fit_transform': True, 'Transform': True,\n",
    "        'inverse_transform': False, 'DecisionFunction': False, 'decode': True, 'get_feature_names': False,\n",
    "        'get_stop_words': True\n",
    "        }\n",
    "    }\n",
    "\n",
    "ClassifiersModels_dict = {\n",
    "    'LogisticRegression': {\n",
    "        'model_type': 'ClassifierModel', 'model': LogisticRegression(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': False, 'predict': True, 'predict_proba': True, \n",
    "        'decision_function': True, 'densify': True, 'sparsify': True, 'score': True, \n",
    "        # 'fit_predict': False, , 'fit_transform': False, 'Transform': False, 'inverse_transform': True, \n",
    "        # 'decode': False, 'get_feature_names': False, 'get_stop_words': False\n",
    "        }, \n",
    "    'LinearSVC': {\n",
    "        'model_type': 'ClassifierModel', 'model': LogisticRegression(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': False, 'predict': True, 'predict_proba': True, \n",
    "        'decision_function': True, 'densify': True, 'sparsify': True, 'score': True\n",
    "        },\n",
    "    'MultinomialNB': {\n",
    "        'model_type': 'ClassifierModel', 'model': MultinomialNB(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': True, 'predict': True, 'predict_proba': True, \n",
    "        'decision_function': False, 'densify': False, 'sparsify': False, 'score': True\n",
    "        },\n",
    "    'BernoulliNB': {\n",
    "        'model_type': 'ClassifierModel', 'model': BernoulliNB(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': True, 'predict': True, 'predict_proba': True, \n",
    "        'decision_function': False, 'densify': False, 'sparsify': False, 'score': True\n",
    "        },\n",
    "    'RidgeClassifier': {\n",
    "        'model_type': 'ClassifierModel', 'model': RidgeClassifier(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': False, 'predict': True, 'predict_proba': False, \n",
    "        'decision_function': True, 'densify': False, 'sparsify': False, 'score': True\n",
    "        },\n",
    "    'AdaBoostClassifier': {\n",
    "        'model_type': 'ClassifierModel', 'model': AdaBoostClassifier(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': False, 'predict': True, 'predict_proba': True, \n",
    "        'decision_function': True, 'densify': False, 'sparsify': False, 'score': True\n",
    "        },\n",
    "    'Perceptron': {\n",
    "        'model_type': 'ClassifierModel', 'model': Perceptron(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': True, 'predict': True, 'predict_proba': False, \n",
    "        'decision_function': True, 'densify': True, 'sparsify': True, 'score': True\n",
    "        },\n",
    "    'NearestCentroid': {\n",
    "        'model_type': 'ClassifierModel', 'model': NearestCentroid(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': False, 'predict': True, 'predict_proba': False, \n",
    "        'decision_function': False, 'densify': False, 'sparsify': False, 'score': True\n",
    "        },\n",
    "    \n",
    "    'XGBClassifier': { ## NOT from sklearn\n",
    "        'model_type': 'ClassifierModel', 'model': XGBClassifier(), 'datatype_boundation': 'Numeric',\n",
    "        'fit': True, 'partial_fit': False, 'predict': True, 'predict_proba': True, \n",
    "        'decision_function': False, 'densify': False, 'sparsify': False, 'score': False\n",
    "        },\n",
    "    \n",
    "    \n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "Model = AnomalyClusteringModels_dict[ClustAlgo]['Model']\n",
    "        Model.set_params(**ClustAlgo_params)\n",
    "        ModelSpecificDataPreparation = AnomalyClusteringModels_dict[ClustAlgo]['DataTypeBoundation']\n",
    "        FeatureScalingID = 'AnomalyClusteringModelsImplementation__' + DimRedAlgo + '_With_' + DimRedAlgo_ParamName + '__' + ClustAlgo + '_With_' + ClustAlgo_ParamName  + '___Data' + ModelSpecificDataPreparation\n",
    "        ModelType = AnomalyClusteringModels_dict[ClustAlgo]['ModelType']\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Vectorizer Links\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\n",
    "    \n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html\n",
    "    https://xgboost.readthedocs.io/en/latest/python/python_api.html\n",
    "    \n",
    "HyperParameters\n",
    "    CountVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64’>)\n",
    "    TfidfVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=’word’, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.float64’>, norm=’l2’, use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    HashingVectorizer(input=’content’, encoding=’utf-8’, decode_error=’strict’, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=’(?u)\\b\\w\\w+\\b’, ngram_range=(1, 1), analyzer=’word’, n_features=1048576, binary=False, norm=’l2’, alternate_sign=True, dtype=<class ‘numpy.float64’>)\n",
    "    \n",
    "    LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None\n",
    "    LinearSVC(penalty=’l2’, loss=’squared_hinge’, dual=True, tol=0.0001, C=1.0, multi_class=’ovr’, fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
    "    MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "    BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)\n",
    "    RidgeClassifier(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver=’auto’, random_state=None)\n",
    "    AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=’SAMME.R’, random_state=None)\n",
    "    Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
    "    NearestCentroid(metric=’euclidean’, shrink_threshold=None)\n",
    "    XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
    "    \n",
    ")\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "DataTransfRedNClustAlgo = { ('LDA', 'LDA_PC') : [('DBSCAN', 'DBSCAN_PC'), ('MeanShift', 'MeanShift_PC'), ('Birch', 'Birch_PC')], ('PCA', 'PCA_PC') : [('KMeans', 'KMeans_PC'), ('IsolationForest', 'IsolationForest_PC')], ('ICA', 'FastICA_PC') : [('IsolationForest', 'IsolationForest_PC')] }\n",
    "\n",
    "KMeans_PC = {'n_clusters':5, 'init':'k-means++', 'n_init': 10, 'max_iter': 300, 'tol': 0.0001}\n",
    "## (n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001, precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm=’auto’)\n",
    "## algorithm : “auto”, “full” or “elkan”, default=”auto”\n",
    "\n",
    "AffinityPropagation_PC = {'damping':0.75, 'max_iter':200, 'convergence_iter':15 }\n",
    "## (damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity=’euclidean’, verbose=False)\n",
    "## affinity : string, optional, default=``euclidean``\n",
    "\n",
    "AgglomerativeClustering_PC = {'n_clusters':3, 'affinity':'euclidean', 'linkage':'ward'}\n",
    "## (n_clusters=2, affinity=’euclidean’, memory=None, connectivity=None, compute_full_tree=’auto’, linkage=’ward’, pooling_func=<function mean>)\n",
    "## linkage : {“ward”, “complete”, “average”}\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------- #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#         if(type(train_df) == pd.core.frame.DataFrame):\n",
    "#             TrainDF = pd.DataFrame.copy(train_df)\n",
    "#             AllFeature = list(TrainDF.columns)\n",
    "#         else:\n",
    "#             TrainDF = None\n",
    "#         if(type(test_df) == pd.core.frame.DataFrame):\n",
    "#             TestDF = pd.DataFrame.copy(test_df)\n",
    "#             AllFeature = list(TestDF.columns)\n",
    "#         else:\n",
    "#             TestDF = None\n",
    "#         FeatureToIgnore = ast.literal_eval(config['DataProcessing_General']['FeatureToIgnore'])\n",
    "#         FeatureToUse = [ j for j in AllFeature if j not in FeatureToIgnore ]\n",
    "\n",
    "        DimRedAlgo = AlgosComb['DimensionTransformation'][0]\n",
    "        ClustAlgo = AlgosComb['AnomalyClustering'][0]\n",
    "        ExtraColor=config['AnomalyClusterConfiguration']['Visual_Extra_Color']\n",
    "\n",
    "        ## Getting the Configuration for the Algorithm\n",
    "        DimRedAlgo_ParamName = AlgosComb['DimensionTransformation'][1]\n",
    "        # DimRedAlgo_params =  ast.literal_eval(config['DataProcessing_Outlier'][DimRedAlgo_ParamName])\n",
    "        ClustAlgo_ParamName = AlgosComb['AnomalyClustering'][1]\n",
    "        ClustAlgo_params =  ast.literal_eval(config['AnomalyClusterConfiguration'][ClustAlgo_ParamName])\n",
    "\n",
    "        ### Defining Models and their property\n",
    "#         AnomalyClusteringModels_dict = {\n",
    "#             'IsolationForest': {'ModelType': 'AnomalyModelData', 'Model': IsolationForest(), 'DataTypeBoundation': 'Nil', \n",
    "#                                 'fit': True, 'fit_predict': False, 'predict': True, 'DecisionFunction': True}, \n",
    "#             'EllipticEnvelope': {'ModelType': 'AnomalyModelData', 'Model': EllipticEnvelope(), 'DataTypeBoundation': 'Normalized', \n",
    "#                                  'fit': True, 'fit_predict': False, 'predict': True, 'DecisionFunction': True}, \n",
    "#             'LocalOutlierFactor': {'ModelType': 'ClusterModelData', 'Model': LocalOutlierFactor(), 'DataTypeBoundation': 'Nil', \n",
    "#                                    'fit': True, 'fit_predict': True, 'predict': False, 'DecisionFunction': False}, \n",
    "#             'OneClassSVM': {'ModelType': 'AnomalyModelData', 'Model': OneClassSVM(), 'DataTypeBoundation': 'Nil', \n",
    "#                             'fit': True, 'fit_predict': False, 'predict': True, 'DecisionFunction': True}, \n",
    "\n",
    "#             'KMeans': {'ModelType': 'ClusterModelData', 'Model': KMeans(), 'DataTypeBoundation': 'Nil', \n",
    "#                                 'fit': True, 'fit_predict': True, 'predict': True, 'DecisionFunction': False}, \n",
    "#             'MiniBatchKMeans': {'ModelType': 'ClusterModelData', 'Model': MiniBatchKMeans(), 'DataTypeBoundation': 'Nil', \n",
    "#                                  'fit': True, 'fit_predict': True, 'predict': True, 'DecisionFunction': False}, \n",
    "#             'AffinityPropagation': {'ModelType': 'ClusterModelData', 'Model': AffinityPropagation(), 'DataTypeBoundation': 'Nil', \n",
    "#                                    'fit': True, 'fit_predict': True, 'predict': True, 'DecisionFunction': False}, \n",
    "#             'MeanShift': {'ModelType': 'ClusterModelData', 'Model': MeanShift(), 'DataTypeBoundation': 'Nil', \n",
    "#                             'fit': True, 'fit_predict': True, 'predict': True, 'DecisionFunction': False},\n",
    "#             'Birch': {'ModelType': 'ClusterModelData', 'Model': Birch(), 'DataTypeBoundation': 'Nil', \n",
    "#                                 'fit': True, 'fit_predict': True, 'predict': True, 'DecisionFunction': False}, \n",
    "#             'SpectralClustering': {'ModelType': 'ClusterModelData', 'Model': SpectralClustering(), 'DataTypeBoundation': 'Nil', \n",
    "#                                  'fit': True, 'fit_predict': True, 'predict': False, 'DecisionFunction': False}, \n",
    "#             'AgglomerativeClustering': {'ModelType': 'ClusterModelData', 'Model': AgglomerativeClustering(), 'DataTypeBoundation': 'Nil', \n",
    "#                                    'fit': True, 'fit_predict': True, 'predict': False, 'DecisionFunction': False}, \n",
    "#             'DBSCAN': {'ModelType': 'ClusterModelData', 'Model': DBSCAN(), 'DataTypeBoundation': 'Nil', \n",
    "#                             'fit': True, 'fit_predict': True, 'predict': False, 'DecisionFunction': False},\n",
    "#         }\n",
    "\n",
    "#         Model = AnomalyClusteringModels_dict[ClustAlgo]['Model']\n",
    "#         Model.set_params(**ClustAlgo_params)\n",
    "#         ModelSpecificDataPreparation = AnomalyClusteringModels_dict[ClustAlgo]['DataTypeBoundation']\n",
    "#         FeatureScalingID = 'AnomalyClusteringModelsImplementation__' + DimRedAlgo + '_With_' + DimRedAlgo_ParamName + '__' + ClustAlgo + '_With_' + ClustAlgo_ParamName  + '___Data' + ModelSpecificDataPreparation\n",
    "#         ModelType = AnomalyClusteringModels_dict[ClustAlgo]['ModelType']\n",
    "\n",
    "        ## Normalizing the Dataset for the algorithm which requires non negative dataset\n",
    "        if ModelSpecificDataPreparation != 'Nil':\n",
    "            if(TrainDF is not None):\n",
    "                TrainDF, _ = DataFrameScaling(TrainDF, FeatureToIgnore, config, FeatureScalingID, ModelSpecificDataPreparation)\n",
    "            ## Test Should Not Change the Stored Values\n",
    "            if(TestDF is not None):\n",
    "                TestDF, _ = DataFrameScaling(TestDF, FeatureToIgnore, config, FeatureScalingID, ModelSpecificDataPreparation, 'GlTest')\n",
    "\n",
    "        print('\\nSeries Anomaly Removal Using', ClustAlgo)\n",
    "        ModelName = config['input']['ModelsSaving_dir'] + FeatureScalingID \n",
    "\n",
    "        ## Training Model\n",
    "        if(TrainDF is not None):\n",
    "            print('Developing and Saving Model :: Training Section :: On provided Training Data')\n",
    "            if(AnomalyClusteringModels_dict[ClustAlgo]['fit_predict'] == True):\n",
    "                TrainDF[ClustAlgo + '_Predict'] = pd.DataFrame( Model.fit_predict(TrainDF[FeatureToUse]) )  \n",
    "            elif((AnomalyClusteringModels_dict[ClustAlgo]['fit'] == True) & (AnomalyClusteringModels_dict[ClustAlgo]['predict'] == True)):\n",
    "                Model.fit(TrainDF[FeatureToUse]) \n",
    "                TrainDF[ClustAlgo + '_Predict'] = pd.DataFrame(Model.predict(TrainDF[FeatureToUse])) \n",
    "            else:\n",
    "                print('Some Error is present')\n",
    "\n",
    "            if(AnomalyClusteringModels_dict[ClustAlgo]['predict'] == False): ## Simply Traning the Model based on the fit_predict data\n",
    "                ## [1]\n",
    "                X = TrainDF[FeatureToUse]\n",
    "                y = TrainDF[ClustAlgo + '_Predict']\n",
    "                Mod = PredictFunction_Alternate(X,y)\n",
    "                ## Saving the model used for predict function locally\n",
    "                joblib.dump(Mod, ModelName+'_AddedExtPredict')\n",
    "            ### Check Which one is more accurate [1] or [2]\n",
    "\n",
    "            ## Generating Score if it can be generated\n",
    "            if(AnomalyClusteringModels_dict[ClustAlgo]['DecisionFunction'] == True):\n",
    "                TrainDF[ClustAlgo + '_Score'] = pd.DataFrame( Model.decision_function(TrainDF[FeatureToUse]) ) \n",
    "                if(AnomalyClusteringModels_dict[ClustAlgo]['predict'] == False):\n",
    "                    ## [2] considering General Boundary as 0, if less tham this value --> anomaly \n",
    "                    TrainDF[ClustAlgo + '_PredBasedOnScore'] = [ 1 if elem >= 0 else -1 for elem in TrainDF[ClustAlgo + '_Score'] ]\n",
    "\n",
    "            ## Saving the Result Locally\n",
    "            WriteOutputFile(TrainDF, ModelType, 'Train', AlgosComb, config)\n",
    "            ## Saving the PreProcessed Result image, first three dimension\n",
    "            if config['TriggerTheseFunctions']['VisualizeClusters'] != 'False':\n",
    "                VisualizeClusters(TrainDF, DimRedAlgo, ClustAlgo, config, {'ax1': 0,'ax2': 1,'ax3': 2}, ExtraColor)\n",
    "            ## Saving the model locally\n",
    "            joblib.dump(Model, ModelName)\n",
    "\n",
    "        ## Using Developed Model \n",
    "        if(TestDF is not None):\n",
    "            print('Using Saved Model :: Predict Section :: On provided Test Data')\n",
    "            ## Loading the locally saved model\n",
    "            Model = joblib.load(ModelName)\n",
    "            if(AnomalyClusteringModels_dict[ClustAlgo]['predict'] == True):\n",
    "                TestDF[ClustAlgo + '_Predict'] = pd.DataFrame( Model.predict(TestDF[FeatureToUse]) )  \n",
    "            elif(AnomalyClusteringModels_dict[ClustAlgo]['predict'] == False):\n",
    "                ## using the model used for predict function locally\n",
    "                Model_pred = joblib.load(ModelName+'_AddedExtPredict')\n",
    "                X = TestDF[FeatureToUse]\n",
    "                TestDF[ClustAlgo + '_Predict'] = Model_pred.predict(X)\n",
    "\n",
    "            if(AnomalyClusteringModels_dict[ClustAlgo]['DecisionFunction'] == True):\n",
    "                TestDF[ClustAlgo + '_Score'] = pd.DataFrame( Model.decision_function(TestDF[FeatureToUse]) ) \n",
    "                if(AnomalyClusteringModels_dict[ClustAlgo]['predict'] == False):\n",
    "                    TestDF[ClustAlgo + '_PredBasedOnScore'] = [ 1 if elem >= 0 else -1 for elem in testDF[ClustAlgo + '_Score'] ]\n",
    "            ## Saving the Result Locally\n",
    "            WriteOutputFile(TestDF, ModelType, 'Test', AlgosComb, config)\n",
    "            ## Saving the PreProcessed Result image, first three dimension\n",
    "            if config['TriggerTheseFunctions']['VisualizeClusters'] != 'False':\n",
    "                VisualizeClusters(TestDF, DimRedAlgo, ClustAlgo, config, {'ax1': 0,'ax2': 1,'ax3': 2}, ExtraColor)\n",
    "    except Exception as e:\n",
    "        print('Error :', str(e))\n",
    "        \n",
    "    EndTime = time.time()\n",
    "    print('Time Taken :', (EndTime - StartTime)/60, ' min.')\n",
    "    return TrainDF, TestDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to Cross Validate Multiple configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Multiple Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Get Vectorizer_Classifiers into a same list and the function will seperate out by itself\n",
    "algo_combos, algo_combos_li = config['ModelPreparation']['ModelsToPrepare'], []\n",
    "grid_algo_combos = config['ModelPreparation']['GridSearchCombinations']\n",
    "\n",
    "for alc in algo_combos.keys():\n",
    "    temp_dict = ast.literal_eval(''.join([ ele for ele in algo_combos[alc].split(' ') if len(ele) >0 ]))\n",
    "    algo_combos_li.append(temp_dict)\n",
    "for alc in grid_algo_combos.keys():\n",
    "    temp_dict = ast.literal_eval(''.join([ ele for ele in grid_algo_combos[alc].split(' ') if len(ele) >0 ]))\n",
    "    algo_combos_li.append(temp_dict)\n",
    "\n",
    "##\n",
    "algo_combos_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "## Vectorizers\n",
    "vect_CountVectorizer = CountVectorizer()\n",
    "vect_TfidfVectorizer = TfidfVectorizer()\n",
    "vect_HashingVectorizer = HashingVectorizer()\n",
    "\n",
    "## Classifiers\n",
    "clss_LogisticRegression = LogisticRegression(solver='lbfgs', max_iter=250)\n",
    "clss_LinearSVC = LinearSVC()\n",
    "clss_MultinomialNB = MultinomialNB()\n",
    "clss_BernoulliNB = BernoulliNB()\n",
    "clss_RidgeClassifier = RidgeClassifier()\n",
    "clss_AdaBoostClassifier = AdaBoostClassifier()\n",
    "clss_Perceptron = Perceptron()\n",
    "clss_NearestCentroid = NearestCentroid()\n",
    "clss_XGB_Classifier = XGBClassifier()\n",
    "# clss_PassiveAggressiveClassifier = PassiveAggressiveClassifier()\n",
    "\n",
    "clss_LinearSVC_l1FeaSelection = Pipeline([\n",
    "    ('feature_selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))), \n",
    "    ('classification', LinearSVC(penalty='l2'))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "vectorizer_classifier = [{'which_vectorizer': vect_CountVectorizer,\n",
    "                          'which_classifier': clss_LogisticRegression,\n",
    "                          'params': {'which_vectorizer__max_features': [i for i in range(10000, 15001, 10000)], \n",
    "                                        'which_vectorizer__stop_words': [None, 'english'], \n",
    "                                        'which_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                                        'which_classifier__fit_intercept': [True]\n",
    "                                       }\n",
    "                         },\n",
    "                         {'which_vectorizer': vect_TfidfVectorizer, \n",
    "                          'which_classifier': clss_LogisticRegression,\n",
    "                          'params': {'which_vectorizer__max_features': [i for i in range(10000, 15001, 10000)], \n",
    "                                        'which_vectorizer__stop_words': [None, 'english'], \n",
    "                                        'which_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                                        'which_classifier__fit_intercept': [True]\n",
    "                                       }\n",
    "                         },\n",
    "                         {'which_vectorizer': vect_HashingVectorizer, \n",
    "                          'which_classifier': clss_LogisticRegression,\n",
    "                          'params': {'which_vectorizer__n_features': [i for i in range(10000, 15001, 10000)], \n",
    "                                        'which_vectorizer__stop_words': [None, 'english'], \n",
    "                                        'which_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                                        'which_classifier__fit_intercept': [True]\n",
    "                                       }\n",
    "                         },\n",
    "\n",
    "                         {'which_vectorizer': vect_TfidfVectorizer,\n",
    "                          'which_classifier': clss_LinearSVC_l1FeaSelection,\n",
    "                          'params': {'which_vectorizer__max_features': [i for i in range(100000, 100001, 10000)], \n",
    "                                        'which_vectorizer__stop_words': [None], \n",
    "                                        'which_vectorizer__ngram_range': [(1, 2)]\n",
    "                                       }\n",
    "                         },\n",
    "\n",
    "                         {'which_vectorizer': vect_TfidfVectorizer,\n",
    "                          'which_classifier': clss_XGB_Classifier,\n",
    "                          'params': {'which_vectorizer__max_features': [i for i in range(100000, 100001, 10000)], \n",
    "                                        'which_vectorizer__stop_words': [None], \n",
    "                                        'which_vectorizer__ngram_range': [(1, 2)]\n",
    "                                       }\n",
    "                         }\n",
    "                         ]\n",
    "\n",
    "all_result_DF, best_result_DF = grid_search_modspairs(train_df['text'], train_df['sentiment_class'], \n",
    "                                                      vectorizer_classifier)\n",
    "display(best_result_DF)\n",
    "print('Total number of models that were trained: {}'.format(len(all_result_DF)))\n",
    "display(all_result_DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# sns.set(rc={'figure.figsize':(11.7,5.27)})\n",
    "# sns.set_style('whitegrid')\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "# sns.set_style('ticks')\n",
    "# fig, ax = plt.subplots()\n",
    "# # the size of A4 paper\n",
    "# fig.set_size_inches(11.7, 8.27)\n",
    "\n",
    "VarToUseInHUE = ['which_vectorizer', 'which_classifier', 'param_which_vectorizer__stop_words', \n",
    "                 'param_which_vectorizer__ngram_range', 'param_which_vectorizer__max_features', \n",
    "                 'param_which_vectorizer__n_features']\n",
    "VarName = ['which_vectorizer', 'which_classifier', 'Stopwords', 'ngram_range', \n",
    "           'max_features', 'n_features']\n",
    "for plot in range(len(VarToUseInHUE)):\n",
    "\n",
    "    # sns.set_style('whitegrid')\n",
    "    MissingValue = 'NA'\n",
    "    if VarName[plot] == 'Stopwords':\n",
    "        MissingValue = 'Not Removed'\n",
    "    if VarName[plot] == 'max_features' or VarName[plot] == 'n_features':\n",
    "        MissingValue = np.nan\n",
    "\n",
    "    %config InlineBackend.figure_format = 'retina'\n",
    "    sns.set()\n",
    "    sns.pairplot(x_vars=['mean_fit_time'], y_vars=['mean_test_score'], data=all_result_DF.fillna(MissingValue), hue= VarToUseInHUE[plot], \n",
    "                 size=5, aspect = 2) #, palette = 'Set2', markers=['o', 's', 'D']\n",
    "    plt.title('Accuracy VS Time Taken - {}'.format(VarName[plot]))\n",
    "    plt.xlabel('Mean Time Taken in Training')\n",
    "    plt.ylabel('Mean Accuracy in Cross Validation')\n",
    "\n",
    "    # plt.legend(bbox_to_anchor=(1, 1), loc=2)\n",
    "    # plt.xticks(rotation=-45)\n",
    "    # plt.ylim(0, None)\n",
    "    plt.xlim(0, None)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bokeh Plot\n",
    "\n",
    "print(\"param: Vectorizer__ngram_range Vectorizer__stop_words Classifier__fit_intercept Vectorizer__max_features'\")\n",
    "from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "from bokeh.models import HoverTool\n",
    "output_file(\"color_scatter.html\")\n",
    "\n",
    "### converting to Creates labels for hover\n",
    "allLi = []\n",
    "for i in all_result_DF[\"params\"]:\n",
    "    ele = []\n",
    "    for k in i.keys():\n",
    "        ele.append(str(i[k]))\n",
    "    allLi.append(\"\\n\".join(ele))\n",
    "# allLi\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x = all_result_DF['mean_fit_time'],\n",
    "    y = all_result_DF['mean_test_score'] * 100,\n",
    "    Param = allLi,\n",
    "))\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"(x,y)\", \"($x, $y)\"),\n",
    "    (\"Param\", \"@Param\"),\n",
    "]) #     (\"index\", \"$index\"),\n",
    "\n",
    "# radii = np.random.random(size=N) * 1.5\n",
    "# colors = [\n",
    "#     \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(50+2*x, 30+2*y)\n",
    "# ]\n",
    "\n",
    "# TOOLS=\"hover,crosshair,pan,wheel_zoom,zoom_in,zoom_out,box_zoom,undo,redo,reset,tap,save,box_select,poly_select,lasso_select,\"\n",
    "p = figure(plot_width=700, plot_height=400, tools= [hover], title=\"Accuracy vs Time Taken\", \n",
    "           y_axis_label='Mean Accuracy in Cross Validation', x_axis_label='Mean Time Taken for Training') #, x_range=(0, 70000), y_range=(0, 100)\n",
    "\n",
    "p.circle('x', 'y', source=source, color = 'red', size=5, alpha=0.6)\n",
    "\n",
    "show(p)\n",
    "\n",
    "# print(\"Parameter Combination for best Score:\", Mods_GridSearchCV.best_params_, \"\\nBest Accuracy:\", 100*Mods_GridSearchCV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# clf1 = LogisticRegression()\n",
    "# clf2 = LinearSVC()\n",
    "# clf3 = MultinomialNB()\n",
    "# clf4 = RidgeClassifier()\n",
    "# clf5 = PassiveAggressiveClassifier()\n",
    "ensemble_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('linReg', clss_LogisticRegression), \n",
    "        ('svc', clss_LinearSVC), \n",
    "        ('mulNB', clss_MultinomialNB), \n",
    "        ('berNB', clss_BernoulliNB), \n",
    "        ('ridge', clss_RidgeClassifier),\n",
    "        ('ada', clss_AdaBoostClassifier),\n",
    "        ('percpt', clss_Perceptron),\n",
    "        ('nCentroid', clss_NearestCentroid),\n",
    "        ('xgb', clss_XGB_Classifier),\n",
    "    ], \n",
    "    voting='hard')\n",
    "\n",
    "# %%time\n",
    "vectorizer_classifier = [{'which_vectorizer': vect_TfidfVectorizer,\n",
    "                          'which_classifier': ensemble_classifier,\n",
    "                          'params': {\n",
    "                              'which_vectorizer__max_features': [i for i in range(10000, 15001, 10000)],\n",
    "                              'which_vectorizer__stop_words': [None, 'english'], \n",
    "                              'which_vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "                              }\n",
    "                          }]\n",
    "\n",
    "ensemble_result_DF, ensemble_best_result_DF = grid_search_modspairs(train_df['text'], train_df['sentiment_class'], \n",
    "                                                                    vectorizer_classifier)\n",
    "\n",
    "display(ensemble_best_result_DF)\n",
    "print(\"Total number of models that were trained(Without including CrossValidation):\", len(ensemble_result_DF))\n",
    "display(ensemble_result_DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above chart, we can see including bigram and trigram boost the model performance both in count vectorizer and TFIDF vectorizer. And for every case of unigram to trigram, TFIDF yields better results than count vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the voting classifier does no better than the simple logistic regression model. Thus later part, I will try to finetune logistic regression model. But before that, I would like to try another method of sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing Model with configured Parameter and testing those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before I run the defined function, let me briefly explain about confusion matrix and classification report. In order to evaluate the performance of a model, there are many different metrics that can be used. Below I will talk in case of binary classification, in which the target variable only has two classes to be predicted. In the case of this project, the classes are either \"negative\" or \"positive\".\n",
    "\n",
    "One obvious measure of performance can be accuracy. It is the number of times the model predicted correctly for the class over the number of the whole data set. But in case of classification, this can be broken down further. Below is a representation of confusion matrix.\n",
    "\n",
    "\n",
    "title\n",
    "\n",
    "In the above matrix, each row represents the instances in an actual class while each column represents the instances in a predicted class, and it can be also presented swapping rows and columns (column for the actual class, row for predicted class). So the accuracy (ACC) I mentioned above can be expressed as below.\n",
    "$${ACC} = \\frac {True Positive + True Negative}{Positive + Negative} = \\frac {True Positive + True Negative}{True Positive + False Positive + True Negative + False Negative}$$\n",
    "\n",
    "When the distribution of the classes in data is well balanced, accuracy can give you a good picture of how the model is performing. But when you have skewed data, for example, one of the class is dominant in your data set, then accuracy might not be enough to evaluate your model. Let's say you have a dataset which contains 80% positive class, and 20% negative class. This means that by predicting every data into the positive class, the model will get 80% accuracy. In this case, you might want to explore further into the confusion matrix and try different evaluation metrics.\n",
    "\n",
    "There can be 9 different metrics, just from the combination of numbers from confusion matrix, but I will talk about two of them in particular, and another metric which combines these two.\n",
    "\n",
    "\"Precision\" (also called Positive Predictive Value) tells you what proportion of data predicted as positive actually is positive. In other words, the proportion of True Positive in the set of all positive predicted data. $${PPV(Precision)} = \\frac {True Positive}{True Positive + False Positive}$$\n",
    "\n",
    "\"Recall\" (also called Sensitivity, Hit Rate, True Positive Rate) tells you what proportion of data that actually is positive were predicted positive. In other words, the proportion of True Positive in the set of all actual positive data.\n",
    "$${TPR(Recall)} = \\frac {True Positive}{Positive} = \\frac {True Positive}{True Positive + False Negative}$$\n",
    "\n",
    "Below is the image of confusion matrix of cancer diagnose. If you think of \"cancer\" as positive class, \"no cancer\" as a negative class, the image explains well how to think of precision and recall in terms of the confusion matrix. title\n",
    "\n",
    "And finally, the F1 score is the harmonic mean of precision and recall. The harmonic mean is a specific type of average, which is used when dealing with averages of units, like rates and ratios. So by calculating the harmonic mean of the two metrics, it will give you a good idea of how the model is performing both in terms of precision and recall. The formula is as below $${F1} = 2\\cdot\\frac {Precision\\cdot Recall}{Precision + Recall}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# http://scikit-learn.org/stable/auto_examples/plot_compare_reduction.html\n",
    "## http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "def varname( var, dir=locals()):\n",
    "    ''' \n",
    "    Printing VariableName as string\n",
    "    \n",
    "    Take any Variable Name that is already declared in the Woring Enviroment and return \n",
    "    the variable name as string    \n",
    "    '''\n",
    "    return [ key for key, val in dir.items() if id( val) == id( var)]\n",
    "\n",
    "\n",
    "\n",
    "def perform_gridsearch_on_modsPair(df_x, df_y, vectorizer_classifier_list):\n",
    "    '''\n",
    "    Perform Grid Search\n",
    "    '''\n",
    "    all_result_df, best_result_df = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    ## Looping Over the Configuration\n",
    "    for i in range(len(vectorizer_classifier_list)):\n",
    "        \n",
    "        ## Adding Info dict\n",
    "        info_dict = {\n",
    "            'grid_search_time': time.time(),\n",
    "            'which_vectorizer': varname(vectorizer_classifier_list[i]['which_vectorizer'])[0],\n",
    "            'which_classifier': varname(vectorizer_classifier_list[i]['which_classifier'])[0],\n",
    "        }\n",
    "        cols = ['grid_search_time', 'which_vectorizer', 'which_classifier']\n",
    "        print('\\nWorking on the following pair:\\n\\tVectorizer: {}\\n\\tClassifier: {}'.format(\n",
    "            info_dict['which_vectorizer'], info_dict['which_classifier']))\n",
    "        \n",
    "        mods_pipe = Pipeline([('which_vectorizer', vectorizer_classifier_list[i]['which_vectorizer']), \n",
    "                              ('which_classifier', vectorizer_classifier_list[i]['which_classifier'])\n",
    "                             ])\n",
    "        \n",
    "        ## Performing Grid Search\n",
    "        param_grid = [vectorizer_classifier_list[i]['params']]\n",
    "        mods_grid_search_cv = GridSearchCV(mods_pipe, param_grid= param_grid, cv=2, n_jobs= -1, \n",
    "                                           iid= False, return_train_score=False, verbose = 1\n",
    "                                          )# , scoring='accuracy'\n",
    "        mods_grid_search_cv.fit(df_x, df_y) ## Won't be using Holdout CrossValidata ##x_ValDF, y_ValDFb\n",
    "        \n",
    "        ## Converting the results to structured format\n",
    "        all_run_tempDF = pd.DataFrame(mods_grid_search_cv.cv_results_)\n",
    "        all_run_tempDF['which_vectorizer'] = [ info_dict['which_vectorizer'] for i in range(len(all_run_tempDF))]\n",
    "        all_run_tempDF['which_classifier'] = [ info_dict['which_classifier'] for i in range(len(all_run_tempDF))]\n",
    "        \n",
    "        ## adding more info to the dictionary\n",
    "        info_dict['grid_search_time'] = str(round(time.time() - info_dict['grid_search_time'], 2)) +' sec' ## won't be meaningful as all grid search time is there -- aggregated\n",
    "        info_dict['best_score'] = mods_grid_search_cv.best_score_,\n",
    "        info_dict['best_parameters'] = str(mods_grid_search_cv.best_params_)\n",
    "        cols += ['best_score', 'best_parameters']\n",
    "        \n",
    "        ## Converting the results to structured format\n",
    "        best_state_tempDF = pd.DataFrame(info_dict, columns= cols, index = [i])\n",
    "        \n",
    "        ## Appending the info if result into previous interation results\n",
    "        if all_result_df.shape == (0,0):\n",
    "            all_result_df = all_run_tempDF.copy()\n",
    "        else:\n",
    "            all_result_df = all_result_df.append(all_run_tempDF, ignore_index=True, sort=False)\n",
    "        \n",
    "        ## Appending the info if result into previous interation results\n",
    "        if best_result_df.shape == (0,0):\n",
    "            best_result_df = best_state_tempDF.copy()\n",
    "        else:\n",
    "            best_result_df = best_result_df.append(best_state_tempDF, ignore_index=True, sort=False)\n",
    "        \n",
    "        print('Time consumed in this grid search: {}'.format(info_dict['grid_search_time'])) #{0:.2f}s\n",
    "    \n",
    "    return mods_grid_search_cv, all_result_df, best_result_df\n",
    "# a, b, c = perform_gridsearch_on_modsPair(train_DF['text'], train_DF['sentiment_class'], vectorizer_classifier)\n",
    "\n",
    "\n",
    "\n",
    "def train_modsPair(trainDF_x, trainDF_y, vectorizer_classifier_list):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    mod_dict = {}\n",
    "    for i in range(len(vectorizer_classifier_list)):\n",
    "        mod_cnt = 'model_'+str(i+1)\n",
    "        ## Adding Info dict\n",
    "        mod_dict[mod_cnt] = {\n",
    "            'training_time': time.time(),\n",
    "            'which_vectorizer': varname(vectorizer_classifier_list[i]['which_vectorizer'])[0],\n",
    "            'which_classifier': varname(vectorizer_classifier_list[i]['which_classifier'])[0],\n",
    "        }\n",
    "        cols = ['training_time', 'which_vectorizer', 'which_classifier']\n",
    "        print('\\nWorking on the following pair:\\n\\tVectorizer: {}\\n\\tClassifier: {}'.format(\n",
    "            mod_dict[mod_cnt]['which_vectorizer'], mod_dict[mod_cnt]['which_classifier']))\n",
    "        \n",
    "        mods_pipe = Pipeline([\n",
    "            ('which_vectorizer', vectorizer_classifier_list[i]['which_vectorizer']), \n",
    "            ('which_classifier', vectorizer_classifier_list[i]['which_classifier'])\n",
    "            ])\n",
    "        \n",
    "        mods_pipe.fit(trainDF_x, trainDF_y) \n",
    "        \n",
    "        mod_dict[mod_cnt]['model'] = mods_pipe\n",
    "        mod_dict[mod_cnt]['training_time'] = str(round(time.time() - mod_dict[mod_cnt]['training_time'], 2)) +' sec'\n",
    "        print('Time Taken: {}'.format(mod_dict[mod_cnt]['training_time'])) #{0:.2f}s\n",
    "        \n",
    "    return mod_dict\n",
    "# a = train_modsPair(train_DF['text'], train_DF['sentiment_class'], vectorizer_classifier)\n",
    "\n",
    "\n",
    "def evaluate_model_performance(specific_dict_format, testDF_x, testDF_y):\n",
    "    '''\n",
    "    Specific to 'perform_operation_on_model_pairs' function\n",
    "    '''\n",
    "    if 'GridSearch' in specific_dict_format.keys():\n",
    "        print('\\nPlotting the Graph for the best Model Trained on GridSearchCV')\n",
    "        y_pred = specific_dict_format['GridSearch']['BestModel'].predict(testDF_x)\n",
    "        plotConfusionMatrix(testDF_y, y_pred)\n",
    "        \n",
    "    if 'Train' in specific_dict_format.keys():\n",
    "        for key in specific_dict_format['Train']:\n",
    "            print('\\nPlotting the graph for the following pair:\\n\\tVectorizer: {}\\n\\tClassifier: {}'.format(\n",
    "            specific_dict_format['Train'][key]['which_vectorizer'], specific_dict_format['Train'][key]['which_classifier']))\n",
    "            y_pred = specific_dict_format['Train'][key]['model'].predict(testDF_x)\n",
    "            plotConfusionMatrix(testDF_y, y_pred)\n",
    "\n",
    "\n",
    "def perform_operation_on_model_pairs(vectorizer_classifier_list, trainDF_x, trainDF_y, \n",
    "                                     testDF_x=None, testDF_y=None, operation=['Train', 'Evaluate']):\n",
    "    '''\n",
    "    Description\n",
    "        Function takes a list which defines, on which all combination to either perform grid search or\n",
    "        train models\n",
    "        \n",
    "        Train/GridSearch: 'params' key in dict present in vectorizer_classifier_list gonna seperate \n",
    "                        these entities\n",
    "        \n",
    "        This Function for now is Specific to Vectorizer and Classifier\n",
    "    \n",
    "    Parameters -------------------------------------------------------------------------------------\n",
    "    Operations: 'Train', 'Evaluate' And 'GridSearch'\n",
    "        If Train: train models present in 'vectorizer_classifier_list' and returns a list \n",
    "                  containing models.\n",
    "        If Evaluate: 'testDF_x' & 'testDF_y' shouldn't be None and the external evaluation \n",
    "                  should be perform on these datasets and print the evaluations\n",
    "        If GridSearch: dict present inside the list 'vectorizer_classifier_list' should \n",
    "                  contain 'params' key and the respective values and returns the configurations\n",
    "        \n",
    "    vectorizer_classifier_list: following format should be followed\n",
    "        [{\n",
    "            'which_vectorizer': vectorizer_instance, \n",
    "            'which_classifier': classifier_instance,\n",
    "            'params': {\n",
    "                    'which_vectorizer__HyperParameter1': [opt1, opt2, opt3],\n",
    "                    'which_vectorizer__HyperParameter2': [opt1, opt2],\n",
    "                    .....\n",
    "                    'which_classifier__HyperParameter1': [opt1, opt2, opt3, opt4],\n",
    "                    .....\n",
    "                    }\n",
    "        }]\n",
    "    \n",
    "    trainDF_x, testDF_x: series of texts\n",
    "    \n",
    "    trainDF_y, testDF_y: label of class\n",
    "    '''\n",
    "    dict_to_return = {}\n",
    "    \n",
    "    ## Check if configurations are okay wrt the operations and defining the variables of interest\n",
    "    if 'Train' in operation:\n",
    "        print('>> Working on Training')\n",
    "        ## hyperparameter shouldn't be passed through params for the training \n",
    "        train_vect_cls_li = [ dct for dct in vectorizer_classifier_list if 'params' not in dct.keys() ]\n",
    "        ## Trained Models\n",
    "        dict_to_return['Train'] = train_modsPair(trainDF_x, trainDF_y, train_vect_cls_li)\n",
    "    \n",
    "    if 'GridSearch' in operation:\n",
    "        print('>> Working on GridSearch')\n",
    "        ## Check if params is present -- Grid Search will be performed on only those instance where params is present\n",
    "        grid_vect_cls_li = [ dct for dct in vectorizer_classifier_list if 'params' in dct.keys() ]\n",
    "        \n",
    "        ## Getting the Hyperparameter config report prepared \n",
    "        best_model, all_result_df, best_result_df = perform_gridsearch_on_modsPair(trainDF_x, trainDF_y, grid_vect_cls_li)\n",
    "        dict_to_return['GridSearch'] = {}\n",
    "        dict_to_return['GridSearch']['BestModel'] = best_model\n",
    "        dict_to_return['GridSearch']['DetailedReport'] = all_result_df\n",
    "        dict_to_return['GridSearch']['BestSnapReport'] = best_result_df \n",
    "        \n",
    "    if 'Evaluate' in operation:\n",
    "        print('>> Working on Evaluate')\n",
    "        if (testDF_x is None) & (testDF_y is None):\n",
    "            msg = 'Test set is not provided. Hence Raising Exception.'\n",
    "            raise Exception(msg)\n",
    "        else:\n",
    "            evaluate_model_performance(dict_to_return, testDF_x, testDF_y)\n",
    "\n",
    "\n",
    "perform_operation_on_model_pairs(vectorizer_classifier, train_DF['text'], train_DF['sentiment_class'], \n",
    "                                 test_DF['text'], test_DF['sentiment_class'], operation=['Train', 'GridSearch', 'Evaluate'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%timeit\n",
    "\n",
    "Cnt_Vec1 = CountVectorizer(max_features=80000)\n",
    "Cnt_Vec2 = CountVectorizer(max_features=70000,ngram_range=(1, 2))\n",
    "Cnt_Vec3 = CountVectorizer(max_features=80000,ngram_range=(1, 3))\n",
    "\n",
    "FinParams_VectorizerClassifier = [{'which_vectorizer': Cnt_Vec1,\n",
    "                                   'which_classifier': clss_LogisticRegression\n",
    "                                   },\n",
    "                                  {'which_vectorizer': Cnt_Vec2,\n",
    "                                   'which_classifier': clss_LogisticRegression\n",
    "                                   },\n",
    "                                  {'which_vectorizer': Cnt_Vec3,\n",
    "                                   'which_classifier': clss_LogisticRegression\n",
    "                                   },\n",
    "                                  ]\n",
    "\n",
    "DevelopAndEvalateModelsPair(train_df['text'], train_df['sentiment_class'], test_df['text'], test_df['sentiment_class'], FinParams_VectorizerClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DevelopAndEvalateModelsPair(FinParams_VectorizerClassifier, x_TrainDF, y_TrainDF, x_TestDF, y_TestDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lexical approach\n",
    "\n",
    "What I have demonstrated above are machine learning approaches to text classification problem, which tries to solve the problem by training classifiers on the labelled data set. Another famous approach to sentiment analysis task is a lexical approach. \"In the lexical approach the definition of sentiment is based on the analysis of individual words and/or phrases; emotional dictionaries are often used: emotional lexical items from the dictionary are searched in the text, their sentiment weights are calculated, and some aggregated weight function is applied.\" http://www.dialog-21.ru/media/1226/blinovpd.pdf\n",
    "\n",
    "In the previous part, I have calculated harmonic mean of \"positive rate CDF\" and \"positive frequency percent CDF\", and these have given me a good representation of positive and negative terms in the corpora. If it successfully filters which terms are important to each class, then this can also be used for prediction in a lexical manner.\n",
    "\n",
    "So I decided to make a simple predictor, which make use of the harmonic mean value I calculated. Below I go through the term frequency calculation, and the steps to get 'pos_normcdf_hmean', but this time I calculated term frequency only from the train set. (* Since I learned that I don't need to transform sparse matrix to dense matrix for term frequency calculation, I computed the frequency directly from sparse matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CntVec = CountVectorizer(stop_words='english',max_features=10000)\n",
    "CntVec.fit(train_DF['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NegativeDoc_matrix = CntVec.transform(train_DF['clean_text'].loc[train_DF['sentiment_class'] == 0])\n",
    "PositiveDoc_matrix = CntVec.transform(train_DF['clean_text'].loc[train_DF['sentiment_class'] == 1])\n",
    "print(type(NegativeDoc_matrix), NegativeDoc_matrix.shape)\n",
    "\n",
    "Negative_tf = np.sum(NegativeDoc_matrix,axis=0)\n",
    "Positive_tf = np.sum(PositiveDoc_matrix,axis=0)\n",
    "print(type(Negative_tf), Negative_tf.shape)\n",
    "\n",
    "Negative = np.squeeze(np.asarray(Negative_tf))\n",
    "Positive = np.squeeze(np.asarray(Positive_tf))\n",
    "print(type(Negative), Negative.shape)\n",
    "\n",
    "TermFreq_DF2 = pd.DataFrame([Negative,Positive], columns=CntVec.get_feature_names()).transpose()\n",
    "print(type(TermFreq_DF2), TermFreq_DF.shape)\n",
    "\n",
    "TermFreq_DF2.rename(columns = {0:'Negative', 1:'Positive'}, inplace= True)\n",
    "TermFreq_DF2['TotalFreq'] = TermFreq_DF2['Negative'] + TermFreq_DF2['Positive']\n",
    "print('DataFrame Shape:', TermFreq_DF2.shape)\n",
    "display(TermFreq_DF2.sort_values(by='TotalFreq', ascending=False).head(15))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TermFreq_DF2['PositiveRate'] = TermFreq_DF2['Positive'] * 1./TermFreq_DF2['TotalFreq']\n",
    "TermFreq_DF2['NegativeRate'] = TermFreq_DF2['Negative'] * 1./TermFreq_DF2['TotalFreq']\n",
    "display(TermFreq_DF2.sort_values(by='PositiveRate', ascending=False).head(10))\n",
    "\n",
    "TermFreq_DF2['PositiveFreq_pct'] = TermFreq_DF2['Positive'] * 1./TermFreq_DF2['Positive'].sum()\n",
    "TermFreq_DF2['NegativeFreq_pct'] = TermFreq_DF2['Negative'] * 1./TermFreq_DF2['Negative'].sum()\n",
    "display(TermFreq_DF2.sort_values(by='PositiveFreq_pct', ascending=False).head(10))\n",
    "\n",
    "from scipy.stats import hmean\n",
    "TermFreq_DF2['Positive_hmean'] = TermFreq_DF2.apply(lambda x: (hmean([x['PositiveRate'], x['PositiveFreq_pct']])\n",
    "                                                                   if x['PositiveRate'] > 0 and x['PositiveFreq_pct'] > 0\n",
    "                                                                   else 0), axis=1)\n",
    "TermFreq_DF2['Negative_hmean'] = TermFreq_DF2.apply(lambda x: (hmean([x['NegativeRate'], x['NegativeFreq_pct']])\n",
    "                                                                   if x['NegativeRate'] > 0 and x['NegativeFreq_pct'] > 0 \n",
    "                                                                   else 0), axis=1)\n",
    "display(TermFreq_DF2.sort_values(by='Positive_hmean', ascending=False).head(10))\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "def normcdf(x):\n",
    "    return norm.cdf(x, x.mean(), x.std())\n",
    "\n",
    "TermFreq_DF2['PositiveRate_normcdf'] = normcdf(TermFreq_DF2['PositiveRate'])\n",
    "TermFreq_DF2['PositiveFreq_pct_normcdf'] = normcdf(TermFreq_DF2['PositiveFreq_pct'])\n",
    "TermFreq_DF2['Positive_normcdf_hmean'] = hmean([TermFreq_DF2['PositiveRate_normcdf'], TermFreq_DF2['PositiveFreq_pct_normcdf']])\n",
    "TermFreq_DF2['NegativeRate_normcdf'] = normcdf(TermFreq_DF2['NegativeRate'])\n",
    "TermFreq_DF2['NegativeFreq_pct_normcdf'] = normcdf(TermFreq_DF2['NegativeFreq_pct'])\n",
    "TermFreq_DF2['Negative_normcdf_hmean'] = hmean([TermFreq_DF2['NegativeRate_normcdf'], TermFreq_DF2['NegativeFreq_pct_normcdf']])\n",
    "display(TermFreq_DF2.sort_values(by='Positive_normcdf_hmean', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The calculation of the positivity score I decided is fairly simple and straightforward. For each word in a document, look it up in the list of 10,000 words I built vocabulary with, and get the corresponding 'pos_normcdf_hmean' value, then for the document calculate the average 'pos_normcdf_hmean' value. If none of the words can be found from the built 10,000 terms, then yields random probability ranging between 0 to 1. And the single value I get for a document is handled as a probability of the document being positive class.\n",
    "\n",
    "Normally, a lexical approach will take many other aspects into the calculation to refine the prediction result, but I will try a very simple model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_hmean = TermFreq_DF2['Positive_normcdf_hmean']\n",
    "\n",
    "y_val_predicted_proba = []\n",
    "for t in x_ValDF:\n",
    "    hmean_scores = [pos_hmean[w] for w in t.split() if w in pos_hmean.index]\n",
    "    if len(hmean_scores) > 0:\n",
    "        prob_score = np.mean(hmean_scores)\n",
    "    else:\n",
    "        prob_score = np.random.random()  ## check for this\n",
    "    y_val_predicted_proba.append(prob_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_hmean['wtf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1 if t > 0.56 else 0 for t in y_val_predicted_proba]\n",
    "plotConfusionMatrix(y_ValDF,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_ValDF,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is not as good as logistic regression with count vectorizer or TFIDF vectorizer, but compared to null accuracy, 25.56% more accurate, and even compared to TextBlob sentiment analysis, my simple custom lexicon model is 15.31% more accurate. This is an impressive result for such a simple calculation and also considering the fact that the 'pos_normcdf_hmean' is calculated only with the training set. This might be useful later for an ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><img src=\"http://sierraexpressmedia.com/wp-content/uploads/2016/01/checkpoint.png\" alt=\"roundtoc\" style=\"float:left;width:60px;height:60px;\"><b>&emsp;Checkpoint 5 Reached</b><a class=\"anchor\" id=\"check5\"></a></h2>\n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Go to TOC](#TOC)  \n",
    "&emsp;&emsp;&emsp;&emsp;&ensp;[Checkpoint_Head](#checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WorkspaceBasedCheckPt(ChPt):\n",
    "    var = %who_ls\n",
    "    ThingsToKeep = ['glob', 'dill', 'np', 'os', 'pd', 'plt', 'sns', 'time', 'WorkspaceBasedCheckPt', 'config', 'ChPt', 'TermFreq_DF', 'TermFreq_DF2', 'test_DF', 'train_DF', 'x_TestDF', 'x_TrainDF', 'x_ValDF', 'y_TestDF', 'y_TrainDF', 'y_ValDF']\n",
    "    print(\"Things that can be removed from workspace\")\n",
    "    print(\", \".join([i for i in var if i not in ThingsToKeep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WorkspaceBasedCheckPt(ChPt):\n",
    "    del(TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WorkspaceBasedCheckPt(ChPt, True)\n",
    "ChPt = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "Before we jump into doc2vec, it will be better to first start by word2vec. \"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\"\n",
    "\n",
    "Word2vec is not a single algorithm but consists of two techniques – CBOW(Continuous bag of words) and Skip-gram model. Both of these techniques learn weights which act as word vector representations. With a corpus, CBOW model predicts the current word from a window of surrounding context words, while Skip-gram model predicts surrounding context words given the current word. In Gensim package, you can specify whether to use CBOW or Skip-gram by passing the argument \"sg\" when implementing Word2Vec. By default (sg=0), CBOW is used. Otherwise (sg=1), skip-gram is employed.\n",
    "\n",
    "For example, let's say we have the following sentence: \"I love dogs\". CBOW model tries to predict the word \"love\" when given \"I\", \"dogs\" as inputs, on the other hand, Skip-gram model tries to predict \"I\", \"dogs\" when given the word \"love\" as input.\n",
    "\n",
    "Below picture represents more formally how these two models work.\n",
    "![]()\n",
    "\n",
    "But what's used as word vectors are actually not the predicted results from these models but the weights of the trained models. By extracting the weights, such a vector comes to represent in some abstract way the ‘meaning’ of a word.\n",
    "\n",
    "Then what is doc2vec? Doc2vec uses the same logic as word2vec, but apply this to the document level. According to Mikolov et al. (2014), \"every paragraph is mapped to a unique vector, represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W. The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context...The paragraph token can be thought of as another word. It acts as a memory that remembers what is missing from the current context – or the topic of the paragraph.\" https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "  \n",
    "![]()\n",
    "  \n",
    "DM: This is the Doc2Vec model analogous to CBOW model in Word2vec. The paragraph vectors are obtained by training a neural network on the task of inferring a centre word based on context words and a context paragraph.\n",
    "\n",
    "DBOW: This is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "I implemented Doc2Vec model using a Python library, Gensim. In case of DM model, I implemented averaging and concatenating. This is inspired by the research paper from Le and Mikolov (2014). In their paper, they have implemented DM model in two different way, using average calculation process for the paragraph matrix, and concatenating calculation method for the paragraph matrix. This has also been shown in Gensim's tutorial.\n",
    "\n",
    "Below are the methods I used to get the vectors for each tweet.\n",
    "\n",
    "DBOW (Distributed Bag of Words)\n",
    "DMC (Distributed Memory Concatenated)\n",
    "DMM (Distributed Memory Mean)\n",
    "DBOW + DMC\n",
    "DBOW + DMM\n",
    "With above vectors, I fit a simple logistic regression model and evaluated the result on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WorkspaceBasedCheckPt(ChPt):\n",
    "    from tqdm import tqdm\n",
    "    tqdm.pandas(desc=\"progress-bar\")\n",
    "    from gensim.models import Doc2Vec\n",
    "    from gensim.models.doc2vec import LabeledSentence\n",
    "    import multiprocessing\n",
    "    from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WorkspaceBasedCheckPt(ChPt):\n",
    "    def labelize_tweets_ug(tweets,label):\n",
    "        result = []\n",
    "        prefix = label\n",
    "        for i, t in zip(tweets.index, tweets):\n",
    "            result.append(LabeledSentence(t.split(), [prefix + '_%s' % i]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training Doc2Vec, I used the whole data set. The rationale behind this is that the doc2vec training is completely unsupervised and thus there is no need to hold out any data, as it is unlabelled. This rationale is inspired by the rationale of Lau and Baldwin (2016) in their research paper \"An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation\" https://arxiv.org/pdf/1607.05368.pdf\n",
    "\n",
    "Also, the same rationale has been applied in the Gensim's Doc2Vec tutorial. In the IMDB tutorial, vector training is occurring on all documents of the data set, including all train/test/dev set. https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_x_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dbow = Doc2Vec(dm=0, size=100, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the developer Radim Řehůřek who created Gensim, \"One caveat of the way this algorithm runs is that, since the learning rate decrease over the course of iterating over the data, labels which are only seen in a single LabeledSentence during training will only be trained with a fixed learning rate. This frequently produces less than optimal results.\"\n",
    "\n",
    "Below iteration implement explicit multiple-pass, alpha-reduction approach with added shuffling. This has been already presented in Gensim's IMDB tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 100)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the DBOW model doesn't learn the meaning of the individual words, but as features to feed to a classifier, it seems like it's doing its job.\n",
    "\n",
    "But the result doesn't seem to excel count vectorizer or Tfidf vectorizer. It might not be a direct comparison since either count vectorizer of Tfidf vectorizer uses a large number of features to represent a tweet, but in this case, a vector for each tweet has only 200 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dbow.save('d2v_model_ug_dbow.doc2vec')\n",
    "model_ug_dbow = Doc2Vec.load('d2v_model_ug_dbow.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Momory (concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmc = Doc2Vec(dm=1, dm_concat=1, size=100, window=2, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmc.alpha -= 0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc = Doc2Vec.load('d2v_model_ug_dmc.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's nice about Doc2Vec is that after training you can retrieve not only document vectors but also individual word vectors as well. Note, however, that a Doc2Vec DBOW model doesn't learn semantic word vectors, so the word vectors you retrieve from pure DBOW model will be the automatic randomly-initialized vectors, with no meaning. But with DM model, you can see the semantic relationship between words. Let's see what word vectors it has learned through training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.most_similar('happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's interesting with DMC model is, somehow it learned all the misspelled version of a word as you can see from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.most_similar(positive=['bigger', 'small'], negative=['big'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model successfully catches the comparative form of \"small\", on feeding the word \"big\" and \"bigger\". The above line of code is like asking the model to add the vectors associated with the word \"bigger\" and \"small\" while subtracting \"big\" is equal to the top result, \"smaller\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmc = get_vectors(model_ug_dmc, x_train, 100)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.save('d2v_model_ug_dmc.doc2vec')\n",
    "model_ug_dmc = Doc2Vec.load('d2v_model_ug_dmc.doc2vec')\n",
    "model_ug_dmc.delete_temporary_training_data(keep_doctags_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Memory (mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmm = Doc2Vec(dm=1, dm_mean=1, size=100, window=4, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmm.build_vocab([x for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_dmm.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmm.alpha -= 0.002\n",
    "    model_ug_dmm.min_alpha = model_ug_dmm.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmm.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmc.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dmm = get_vectors(model_ug_dmm, x_train, 100)\n",
    "validation_vecs_dmm = get_vectors(model_ug_dmm, x_validation, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_dmm.save('d2v_model_ug_dmm.doc2vec')\n",
    "model_ug_dmm = Doc2Vec.load('d2v_model_ug_dmm.doc2vec')\n",
    "model_ug_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I have the document vectors from four different models, now I can concatenate them in combination to see how it affects the performance. Below I defined a simple function to concatenate document vectors from different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_train, 200)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_train, 200)\n",
    "validation_vecs_dbow_dmm = get_concat_vectors(model_ug_dbow,model_ug_dmm, x_validation, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(validation_vecs_dbow_dmm, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of unigram, concatenating document vectors in different combination boosted the model performance. The best validation accuracy I got from a single model is from DBOW at 73.89%. With concatenated vectors, I get the highest validation accuracy of 75.51% with DBOW+DMM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import hidden\n",
    "import sqlite3\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(hidden.consumer_key, hidden.consumer_secret)\n",
    "auth.set_access_token(hidden.token_key, hidden.token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = api.geo_search(query=\"London\", granularity=\"city\")\n",
    "\n",
    "place_id_L = places[0].id\n",
    "print('London id is: ',place_id_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = api.geo_search(query=\"Brighton\", granularity=\"city\")\n",
    "\n",
    "place_id_B = places[0].id\n",
    "print('Brighton id is: ',place_id_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = api.geo_search(query=\"Edinburgh\", granularity=\"city\")\n",
    "\n",
    "place_id_E = places[0].id\n",
    "print('Edinburgh id is: ',place_id_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxitems = 10\n",
    "print \"London tweets retrieve testing\"\n",
    "print '----------------------------------'\n",
    "for tweet in tweepy.Cursor(api.search, q=\"place:%s\" % place_id_L).items(maxitems):\n",
    "    print tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxitems = 10\n",
    "print \"Brighton tweets retrieve testing\"\n",
    "print '----------------------------------'\n",
    "for tweet in tweepy.Cursor(api.search, q=\"place:%s\" % place_id_B).items(maxitems):\n",
    "    print tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxitems = 10\n",
    "print \"Edinburgh tweets retrieve testing\"\n",
    "print '---------------------------------'\n",
    "for tweet in tweepy.Cursor(api.search, q=\"place:%s\" % place_id_E).items(maxitems):\n",
    "    print tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('twitter_testing.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.executescript('''\n",
    "\n",
    "CREATE TABLE Tweets_London (\n",
    "    id     INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT UNIQUE,\n",
    "    user_id TEXT,\n",
    "    user_name TEXT,\n",
    "    user_timezone TEXT,\n",
    "    user_language TEXT,\n",
    "    detected_language TEXT,\n",
    "    tweet_text  TEXT,\n",
    "    tweet_created TEXT\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in tweepy.Cursor(api.search, q=\"place:%s\" % place_id_L).items(maxitems):\n",
    "    detected = detect(tweet.text)\n",
    "    cur.execute('''INSERT OR IGNORE INTO Tweets_London (\n",
    "        user_id, user_name, user_timezone, user_language, detected_language, tweet_text, tweet_created\n",
    "        ) \n",
    "    VALUES ( ?,?,?,?,?,?,? )''', (tweet.user.id,tweet.user.screen_name,tweet.user.time_zone,tweet.user.lang,detected,tweet.text,tweet.created_at))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_sql = pd.read_sql_query(\"SELECT * FROM Tweets_London;\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ComputeAccuracy(Mod_Pipeline, train_x, train_y, test_x, test_y):\n",
    "#     ## Computing the baseline accuracy\n",
    "#     BaselineAcc = len(test_x[test_y == 0]) / (len(test_y)*1.)\n",
    "#     if BaselineAcc < 0.5:\n",
    "#         BaselineAcc = 1. - BaselineAcc\n",
    "#     else:\n",
    "#         BaselineAcc = BaselineAcc\n",
    "    \n",
    "#     t0 = time()\n",
    "#     ## Fitting Models that were provided with pipeline\n",
    "#     Models_SequentialFit = Mod_Pipeline.fit(train_x, train_y)\n",
    "#     pred_y = Models_SequentialFit.predict(test_x)\n",
    "#     ## Computing the Time Consumed\n",
    "#     TrainPred_Time = time() - t0\n",
    "#     accuracy = accuracy_score(test_x, pred_y)\n",
    "\n",
    "#     print(\"Baseline accuracy: {0:.2f}%\".format(BaselineAcc*100))\n",
    "#     print(\"Accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    \n",
    "#     if accuracy > null_accuracy:\n",
    "#         print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-BaselineAcc)*100))\n",
    "#     elif accuracy == null_accuracy:\n",
    "#         print(\"model has the same accuracy with the null accuracy\")\n",
    "#     else:\n",
    "#         print(\"model is {0:.2f}% less accurate than null accuracy\".format((BaselineAcc-accuracy)*100))\n",
    "    \n",
    "#     print(\"train and test time: {0:.2f}s\".format(TrainPred_Time))\n",
    "#     print(\"-\"*80)\n",
    "#     return accuracy, TrainPred_Time\n",
    "\n",
    "# cvec = CountVectorizer()\n",
    "# lr = LogisticRegression()\n",
    "# n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "# def nfeature_accuracy_checker(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "#     result = []\n",
    "#     print(classifier)\n",
    "#     print(\"\\n\")\n",
    "#     for n in n_features:\n",
    "#         vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "#         checker_pipeline = Pipeline([ ('vectorizer', vectorizer), ('classifier', classifier) ])\n",
    "        \n",
    "#         print(\"Validation result for {} features\".format(n))\n",
    "#         nfeature_accuracy, time_consumed = ComputeAccuracy(checker_pipeline, x_TrainDF, y_TrainDF, x_ValDF, y_ValDF)\n",
    "#         result.append((n, nfeature_accuracy, time_consumed))\n",
    "#     return result\n",
    "\n",
    "# %%time\n",
    "# print(\"RESULT FOR UNIGRAM WITHOUT STOP WORDS BEING PRESENT IN THE DATA\\n\")\n",
    "# feature_result_wosw = nfeature_accuracy_checker(stop_words='english')\n",
    "\n",
    "\n",
    "# tvec = TfidfVectorizer()\n",
    "# def classifier_comparator(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "#     result = []\n",
    "#     vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "#     for n,c in classifier:\n",
    "#         checker_pipeline = Pipeline([\n",
    "#             ('vectorizer', vectorizer),\n",
    "#             ('classifier', c)\n",
    "#         ])\n",
    "#         print(\"Validation result for {}\".format(n))\n",
    "#         print(c)\n",
    "#         clf_accuracy,tt_time = accuracy_summary(checker_pipeline, x_TrainDF, y_TrainDF, x_ValDF, y_ValDF)\n",
    "#         result.append((n,clf_accuracy,tt_time))\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# names = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\",\"Multinomial NB\", \n",
    "#          \"Bernoulli NB\", \"Ridge Classifier\", \"AdaBoost\", \"Perceptron\",\"Passive-Aggresive\", \"Nearest Centroid\"]\n",
    "# classifiers = [\n",
    "#     LogisticRegression(),\n",
    "#     LinearSVC(),\n",
    "#     Pipeline([\n",
    "#   ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "#   ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "#     MultinomialNB(),\n",
    "#     BernoulliNB(),\n",
    "#     RidgeClassifier(),\n",
    "#     AdaBoostClassifier(),\n",
    "#     Perceptron(),\n",
    "#     PassiveAggressiveClassifier(),\n",
    "#     NearestCentroid()\n",
    "#     ]\n",
    "# zipped_clf = zip(names,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set figure size with matplotlib\n",
    "# plt.figure(figsize=(10,6))\n",
    "\n",
    "# pkmn_type_colors = ['#78C850',  # Grass\n",
    "#                     '#F08030',  # Fire\n",
    "#                     '#6890F0',  # Water\n",
    "#                     '#A8B820',  # Bug\n",
    "#                     '#A8A878',  # Normal\n",
    "#                     '#A040A0',  # Poison\n",
    "#                     '#F8D030',  # Electric\n",
    "#                     '#E0C068',  # Ground\n",
    "#                     '#EE99AC',  # Fairy\n",
    "#                     '#C03028',  # Fighting\n",
    "#                     '#F85888',  # Psychic\n",
    "#                     '#B8A038',  # Rock\n",
    "#                     '#705898',  # Ghost\n",
    "#                     '#98D8D8',  # Ice\n",
    "#                     '#7038F8',  # Dragon\n",
    "#                    ]\n",
    "\n",
    "# # Violin plot with Pokemon color palette\n",
    "# sns.violinplot(x='Type 1', y='Attack', data=df, \n",
    "#                palette=pkmn_type_colors) # Set color palette\n",
    "\n",
    "# # Swarm plot with Pokemon color palette\n",
    "# sns.swarmplot(x='Type 1', y='Attack', data=df, \n",
    "#               palette=pkmn_type_colors)\n",
    "\n",
    "# # Violin plot\n",
    "# sns.violinplot(x='Type 1', y='Attack', data=df)\n",
    "\n",
    "# # sns.boxplot(data=df)\n",
    "# # Plot using Seaborn\n",
    "# sns.lmplot(x='Attack', y='Defense', data=df,\n",
    "#            fit_reg=False, \n",
    "#            hue='Stage', legend=False, palette=\"Set2\")\n",
    "\n",
    "# # sns.plt.show()\n",
    "# sns.axes_style()\n",
    "# sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "# {'axes.axisbelow': True,\n",
    "#  'axes.edgecolor': '.8',\n",
    "#  'axes.facecolor': 'white',\n",
    "#  'axes.grid': True,\n",
    "#  'axes.labelcolor': '.15',\n",
    "#  'axes.linewidth': 1.0,\n",
    "#  'figure.facecolor': 'white',\n",
    "#  'font.family': [u'sans-serif'],\n",
    "#  'font.sans-serif': [u'Arial',\n",
    "#   u'DejaVu Sans',\n",
    "#   u'Liberation Sans',\n",
    "#   u'Bitstream Vera Sans',\n",
    "#   u'sans-serif'],\n",
    "#  'grid.color': '.8',\n",
    "#  'grid.linestyle': u'-',\n",
    "#  'image.cmap': u'rocket',\n",
    "#  'legend.frameon': False,\n",
    "#  'legend.numpoints': 1,\n",
    "#  'legend.scatterpoints': 1,\n",
    "#  'lines.solid_capstyle': u'round',\n",
    "#  'text.color': '.15',\n",
    "#  'xtick.color': '.15',\n",
    "#  'xtick.direction': u'out',\n",
    "#  'xtick.major.size': 0.0,\n",
    "#  'xtick.minor.size': 0.0,\n",
    "#  'ytick.color': '.15',\n",
    "#  'ytick.direction': u'out',\n",
    "#  'ytick.major.size': 0.0,\n",
    "#  'ytick.minor.size': 0.0}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_and_evaluate(pipeline, x_train, y_train, x_test, y_test):\n",
    "#     if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "#         null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "#     else:\n",
    "#         null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "#     sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "#     y_pred = sentiment_fit.predict(x_test)\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "#     confusion = pd.DataFrame(conmat, index=['negative', 'positive'],\n",
    "#                          columns=['predicted_negative','predicted_positive'])\n",
    "#     print(\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "#     print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "#     if accuracy > null_accuracy:\n",
    "#         print(\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "#     elif accuracy == null_accuracy:\n",
    "#         print(\"model has the same accuracy with the null accuracy\")\n",
    "#     else:\n",
    "#         print(\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"Confusion Matrix\\n\")\n",
    "#     print(confusion)\n",
    "#     print(\"-\"*80)\n",
    "#     print(\"Classification Report\\n\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=['negative','positive']))\n",
    "\n",
    "# %%time\n",
    "# ug_cvec = CountVectorizer(max_features=80000)\n",
    "# ug_pipeline = Pipeline([ ('vectorizer', ug_cvec), ('classifier', lr) ])\n",
    "# train_test_and_evaluate(ug_pipeline, x_TrainDF, y_TrainDF, x_ValDF, y_ValDF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyOfficeEnv",
   "language": "python",
   "name": "myofficeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
